{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tulasi javvadi\\chat_with_PDF\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Tulasi javvadi\\chat_with_PDF\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Tulasi javvadi\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to scrape website content\n",
    "def scrape_website(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract text content (simplified example)\n",
    "    paragraphs = soup.find_all('p')\n",
    "    text_content = [p.get_text() for p in paragraphs]\n",
    "    \n",
    "    return text_content\n",
    "\n",
    "# Convert the scraped content into embeddings\n",
    "def create_embeddings(content):\n",
    "    embeddings = model.encode(content)\n",
    "    return embeddings\n",
    "\n",
    "# Store embeddings in FAISS vector database\n",
    "def store_embeddings_in_faiss(embeddings, content):\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)  # L2 distance for similarity\n",
    "    index.add(np.array(embeddings))\n",
    "    \n",
    "    # Optionally store content metadata (e.g., text) for reference\n",
    "    return index, content\n",
    "\n",
    "# Example website scraping\n",
    "urls = [\n",
    "    'https://www.uchicago.edu/',\n",
    "    'https://www.washington.edu/',\n",
    "    'https://www.stanford.edu/',\n",
    "    'https://und.edu/'\n",
    "]\n",
    "\n",
    "# Scraping and processing each URL\n",
    "content_list = []\n",
    "for url in urls:\n",
    "    scraped_content = scrape_website(url)\n",
    "    embeddings = create_embeddings(scraped_content)\n",
    "    index, content = store_embeddings_in_faiss(embeddings, scraped_content)\n",
    "    content_list.append(content)\n",
    "\n",
    "# Save FAISS index for later use\n",
    "faiss.write_index(index, \"website_content.index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid index: 17\n",
      "Invalid index: 10\n",
      "[['20241217T020920Z-16ccd9d5bb924x6khC1MAAs95s00000018700000000002ns']]\n"
     ]
    }
   ],
   "source": [
    "def query_to_embedding(query):\n",
    "    return model.encode([query])\n",
    "\n",
    "def retrieve_relevant_chunks(query_embedding, index, top_k=3):\n",
    "    # Perform similarity search\n",
    "    D, I = index.search(np.array(query_embedding), k=top_k)  # Retrieve top K results\n",
    "    \n",
    "    # Check if there are valid results\n",
    "    if I.shape[1] == 0 or len(I[0]) == 0:\n",
    "        return []  # Return empty if no results found\n",
    "    \n",
    "    # Ensure that the indices are within range of content_list\n",
    "    relevant_chunks = []\n",
    "    for i in I[0]:\n",
    "        if i < len(content_list):  # Check if the index is valid\n",
    "            relevant_chunks.append(content_list[i])\n",
    "        else:\n",
    "            print(f\"Invalid index: {i}\")\n",
    "    \n",
    "    return relevant_chunks\n",
    "\n",
    "\n",
    "# Example Query\n",
    "user_query = \"What is the main focus of research at the University of Washington?\"\n",
    "\n",
    "query_embedding = query_to_embedding(user_query)\n",
    "relevant_chunks = retrieve_relevant_chunks(query_embedding, index)\n",
    "\n",
    "# Display relevant chunks\n",
    "print(relevant_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-large and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flat Chunks: ['20241217T020920Z-16ccd9d5bb924x6khC1MAAs95s00000018700000000002ns']\n",
      "Answer: 20241217\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize a question-answering pipeline using a pre-trained model\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"facebook/bart-large\")\n",
    "\n",
    "def generate_response(query, relevant_chunks):\n",
    "    # Flatten relevant_chunks to a single list of strings if it's nested\n",
    "    flat_chunks = [item for sublist in relevant_chunks for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "    print(\"Flat Chunks:\", flat_chunks)\n",
    "\n",
    "    # Combine the chunks into a single context string\n",
    "    context = \" \".join(flat_chunks)  # Join all chunks into a single context\n",
    "    response = qa_pipeline(question=query, context=context)\n",
    "    \n",
    "    return response['answer']\n",
    "\n",
    "\n",
    "# Generate the response for the user query\n",
    "response = generate_response(user_query, relevant_chunks)\n",
    "print(\"Answer:\", response)\n",
    "relevant_chunks = [[\"This is chunk 1.\"], [\"This is chunk 2.\"]]\n",
    "flat_chunks = [\"This is chunk 1.\", \"This is chunk 2.\"]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
